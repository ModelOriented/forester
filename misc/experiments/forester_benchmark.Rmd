---
title: "forester_benchmark"
author: "Hubert Ruczy≈Ñski"
date: "`r Sys.Date()`"
output: html_document
---

# Downloads

The necessary downloads required for the forester package to work properly, if downloaded, the user can skip this part.

```{r}
install.packages("devtools")
devtools::install_github("ModelOriented/forester")
devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
devtools::install_github('ricardo-bion/ggradar', dependencies = TRUE)
install.packages('tinytex')
tinytex::install_tinytex()
```

To install the h2o package, we recommend to see the resources from its authors under the link: http://h2o-release.s3.amazonaws.com/h2o/master/1735/docs-website/Ruser/Rinstall.html, however the basic option is to run the code below. Note that the users need the Java installed on the machine.

```{r}
install.packages("h2o")
```


# Initialization

Importing the necessary libraries, let's notice that the user has to install the H2O package before, and to do that one has to have Java installed on the machine.

```{r}
library(OpenML)
library(forester)
library(h2o)
library(tictoc)
```

# OpenML-CC18 preparations

## Getting the datasets

Downloading whole OpenML-CC18 benchmark and saving it in the working directory.

```{r}
i          <- 0
bsuite     <- getOMLStudy('OpenML-CC18')
task.ids   <- extractOMLStudyIds(bsuite, 'task.id')
CC18       <- list()
df_names   <- c() 
for (task.id in task.ids) {
  task <- getOMLTask(task.id)
  data <- as.data.frame(task)
  i    <- i + 1
  cat('Data set id: ', i, 'Data set name: ', task$input$data.set$desc$name, '\n')
  df_names  <- c(df_names, task$input$data.set$desc$name)
  CC18[[i]] <- data
}
names(CC18) <- df_names
saveRDS(CC18, file = "CC18.RData")
```

## Data sets exploration

Data sets exploration function provides us with some basic information about the data sets used in a benchmark, such as number of rows, columns, target values, task name, and data set id.

```{r}
datasets_exploration <- function(tasks_data) {
  df_id       <- c()
  y_name      <- c()
  df_name     <- c()
  y_nr_levels <- c()
  ncols       <- c()
  nrows       <- c()
  for (i in 1:length(tasks_data)) {
    df_id       <- c(df_id, i)
    df_name     <- c(df_name, names(tasks_data)[i])
    y_name      <- c(y_name, colnames(tasks_data[[i]])[ncol(tasks_data[[i]])])
    y_nr_levels <- c(y_nr_levels, length(levels(tasks_data[[i]][, ncol(tasks_data[[i]])])))
    ncols       <- c(ncols, ncol(tasks_data[[i]]))
    nrows       <- c(nrows, nrow(tasks_data[[i]]))
  }
  
  df_exploration <- data.frame(df_id, df_name, y_nr_levels, ncols, nrows, y_name)
  return(df_exploration)
}
```

## Exploration of the CC18

```{r}
exploration <- datasets_exploration(CC18)
cat('Summary of ncols: \n')
summary(exploration$ncols)
cat('Summary of nrows: \n')
summary(exploration$nrows)
exploration
```

## Binary datasets selection

As the forester package doesn't work on the multiclass classification, we have to limit ourselves to the binary classifications tasks only, and save the dataset in a working directory, so we don't have to download everything again.

```{r}
CC18              <- readRDS("CC18.RData")
binary_CC18       <- list()
binary_CC18_names <- c()
j <- 1

for (i in 1:length(CC18)) {
  if (length(levels(CC18[[i]][, ncol(CC18[[i]])])) == 2) {
    binary_CC18[[j]]  <- CC18[[i]]
    binary_CC18_names <- c(binary_CC18_names, names(CC18)[i])
    j <- j + 1
  }
}

names(binary_CC18) <- binary_CC18_names
saveRDS(binary_CC18, file = "binary_CC18.RData")
```

## Exploration of the binary CC18

```{r}
binary_CC18 <- readRDS("binary_CC18.RData")
exploration <- datasets_exploration(binary_CC18)
cat('Summary of ncols: \n')
summary(exploration$ncols)
cat('Summary of nrows: \n')
summary(exploration$nrows)
exploration[c(1, 2, 3, 4, 5, 19, 25, 26), c(2, 4, 5)]
```

# Experiments on the forester package

## Experiment function

The experiment function conducts a model training for the forester package. It trains the `ranger`, `decision_tree`, `lightgbm`, and `catboost` models for every dataset provided to the function. The training is repeated `reps` (=3) times, and we have to provide a `split_seed` vector of length `reps` which sets random seed for the splitting method inside the forester. Every training consists of 10 Bayesian Optimisation iterations and 10 random search evaluations. This function measures the times needed for the training of every model, so we can provide that for the h2o AutoML. the function returns the id's of the split, all models metrics on every subset and best models metrics on every subset.

```{r}
forester_experiment <- function(df_set, split_seed, bayes_iter = 10, random_evals = 10, advanced_preprocessing = FALSE, reps = 3) {
  
  best_models_train <- data.frame()
  best_models_test  <- data.frame()
  best_models_valid <- data.frame()
  
  all_models_train  <- list()
  all_models_test   <- list()
  all_models_valid  <- list()
  
  train_test_valid  <- list()
  
  list_names <- c()
  
  list_times  <- c()
  names_times <- c()
  
  tic('\n General')
  for (i in 1:length(df_set)) {
    tic(paste0('\n Iteration: ', i, ' Dataset: ', names(df_set)[i]))
    for (j in 1:reps) {
      list_times  <- c(list_times, Sys.time())
      names_times <- c(names_times, paste0('Training: ', names(df_set)[i], ' rep ', j))
      output <- train(data = df_set[[i]], 
                      y = colnames(df_set[[i]])[length(df_set[[i]])],
                      engine = c("ranger", "decision_tree", "lightgbm", "catboost"),
                      verbose = FALSE,
                      train_test_split = c(0.6, 0.2, 0.2),
                      split_seed = split_seed[j],
                      bayes_iter = bayes_iter,
                      random_evals = random_evals,
                      advanced_preprocessing = advanced_preprocessing)
      
      top_score_train            <- output$score_train[1, ]
      top_score_train$task_name  <- names(df_set)[i]
      top_score_train$rep        <- j
      
      top_score_test             <- output$score_test[1, ]
      top_score_test$task_name   <- names(df_set)[i]
      top_score_test$rep         <- j
      
      top_score_valid            <- output$score_valid[1, ]
      top_score_valid$task_name  <- names(df_set)[i]
      top_score_valid$rep        <- j
      
      best_models_train <- rbind(best_models_train, top_score_train)
      best_models_test  <- rbind(best_models_test, top_score_test)
      best_models_valid <- rbind(best_models_valid, top_score_valid)
      
      list_names <- c(list_names, paste0(names(df_set)[i], '-rep-', j))
      
      all_models_train[[(i - 1) * reps + j]]  <- data.frame(output$score_train)
      all_models_test[[(i - 1) * reps + j]]   <- data.frame(output$score_train)
      all_models_valid[[(i - 1) * reps + j]]  <- data.frame(output$score_test)
      
      train_test_valid[[(i - 1) * reps + j]]  <- list(
        train_inds = output$train_inds,
        test_inds  = output$test_inds,
        valid_inds = output$valid_inds
      )

    }
    toc()
  }
  list_times  <- c(list_times, Sys.time())
  names_times <- c(names_times, paste0('The end'))
  
  time_df <- data.frame(name = names_times, timestamp = list_times)
  
  print(list_names)
  names(all_models_train)  <- list_names
  names(all_models_test)   <- list_names
  names(all_models_valid)  <- list_names
  names(train_test_valid)  <- list_names
  toc()
  return(list(
    best_models_train = best_models_train,
    best_models_test  = best_models_test,
    best_models_valid = best_models_valid,
    all_models_train  = all_models_train,
    all_models_test   = all_models_test,
    all_models_valid  = all_models_valid,
    train_test_valid  = train_test_valid,
    time_df           = time_df
  ))
}
```

## Batch training

As training the models on the dataset is extremely time consuming (mainly because of MICE imputation and Bayesian Optimisation), we've decided to train the forester in batches, and saving the outcomes in the working directory.

```{r}
outcomes1 <- forester_experiment(head(binary_CC18, 3), c(123, 2137, 21), 10, 10, reps = 3)
saveRDS(outcomes1, file = "trained_1_3_sets_3_reps_10b_10r.RData")
```

```{r}
outcomes2 <- forester_experiment(head(binary_CC18[4:5], 3), c(123, 2137, 21), 10, 10, reps = 3)
saveRDS(outcomes2, file = "trained_4_5_sets_3_reps_10b_10r.RData")
```

```{r}
outcomes3 <- forester_experiment(head(binary_CC18[19], 3), c(123, 2137, 21), 10, 10, reps = 3)
saveRDS(outcomes3, file = "trained_19_sets_3_reps_10b_10r.RData")
```

```{r}
outcomes4 <- forester_experiment(head(binary_CC18[25:26], 3), c(123, 2137, 21), 10, 10, reps = 3)
saveRDS(outcomes4, file = "trained_25_26_sets_3_reps_10b_10r.RData")
```


# Experiments on the H2O package

## Evaluation metrics

```{r}
model_performance_auc <- function(predicted, observed) {
  tpr_tmp <- tapply(observed, predicted, sum)
  TPR     <- c(0, cumsum(rev(tpr_tmp))) / sum(observed)
  fpr_tmp <- tapply(1 - observed, predicted, sum)
  FPR     <- c(0, cumsum(rev(fpr_tmp))) / sum(1 - observed)
  auc     <- sum(diff(FPR) * (TPR[-1] + TPR[-length(TPR)]) / 2)
  
  return(auc)
}

model_performance_recall <- function(tp, fp, tn, fn) {
  return(tp / (tp + fn))
}

model_performance_precision <- function(tp, fp, tn, fn) {
  return(tp / (tp + fp))
}

model_performance_f1 <- function(tp, fp, tn, fn) {
  recall    <- tp / (tp + fn)
  precision <- tp / (tp + fp)
  return(2 * (precision * recall) / (precision + recall))
}

model_performance_accuracy <- function(tp, fp, tn, fn) {
  return((tp + tn) / (tp + fp + tn + fn))
}
```

## Experiment function

The experiments on the h2o consisted of giving them the same, unprocessed data sets as for the forester, and training the h2o 3 times with basic parameters and the runtimes equal to the ones needed by the forester. As the outcomes we obtain the metrics for the best models from the h2o package.

```{r}
h2o_experiment <- function(out_object, runtimes, dataset, reps, limit = 99999) {
  
  localH2O = h2o.init()
  
  best_models_train <- data.frame()
  best_models_test  <- data.frame()
  best_models_valid <- data.frame()
  
  list_times  <- c()
  names_times <- c()
  
  for (i in 1:min(length(out_object$train_test_valid), limit)) {
    list_times  <- c(list_times, Sys.time())
    names_times <- c(names_times, paste0('Training ', i, ' h2o.'))
    j     <- ceiling(i / reps)
    split <- list(
      train = dataset[[j]][out_object$train_test_valid[[i]]$train_inds,],
      test  = dataset[[j]][out_object$train_test_valid[[i]]$test_inds,],
      valid = dataset[[j]][out_object$train_test_valid[[i]]$valid_inds,]
    )
    
    train_H2O <- as.h2o(split$train)
    test_H2O  <- as.h2o(split$test)
    valid_H2O <- as.h2o(split$valid)
    
    y <- colnames(dataset[[j]])[ncol(dataset[[j]])]
    x <- setdiff(names(train_H2O), y)
    
    aml <- h2o.automl(x = x, 
                      y = y,
                      training_frame = train_H2O,
                      max_runtime_secs = runtimes[i])
    
    pred_train <- h2o.predict(aml, train_H2O)
    pred_test  <- h2o.predict(aml, test_H2O)
    pred_valid <- h2o.predict(aml, valid_H2O)
    
    eval <- function(pred, obs, dataset, i) {
      observed    <- as.integer(as.factor(as.vector(obs[, length(obs)])))
      predictions <- as.integer(as.factor(as.vector(pred[, 1])))
      accuracy    <- sum(pred[, 1] == obs[, length(obs)]) / nrow(obs)
      auc         <- model_performance_auc(predictions - 1, observed - 1)
      task_name   <- dataset$best_models_train$task_name[i]
      rep         <- dataset$best_models_train$rep[i]
      
      tp <- sum((observed == 2) * (as.numeric(unlist(predictions)) == 2))
      fp <- sum((observed == 1) * (as.numeric(unlist(predictions)) == 2))
      tn <- sum((observed == 1) * (as.numeric(unlist(predictions)) == 1))
      fn <- sum((observed == 2) * (as.numeric(unlist(predictions)) == 1))
      f1 <- model_performance_f1(tp, fp, tn, fn)
      
      df <- data.frame(task_name = task_name, rep = rep, accruacy = accuracy, auc = auc, f1 = f1)
      return(df)
    }

    best_models_train <- rbind(best_models_train, eval(pred_train, train_H2O, out_object, i))
    best_models_test  <- rbind(best_models_test, eval(pred_test, test_H2O, out_object, i))
    best_models_valid <- rbind(best_models_valid, eval(pred_valid, valid_H2O, out_object, i))
  }
  
  list_times  <- c(list_times, Sys.time())
  names_times <- c(names_times, paste0('The end'))
  
  time_df <- data.frame(name = names_times, timestamp = list_times)
  
  return(list(
    best_models_train = best_models_train,
    best_models_test  = best_models_test,
    best_models_valid = best_models_valid,
    time_df           = time_df
  ))
}
```

# Outcomes

```{r}
out1 <- readRDS("trained_1_3_sets_3_reps_10b_10r.RData")
out2 <- readRDS("trained_4_5_sets_3_reps_10b_10r.RData")
out3 <- readRDS("trained_19_sets_3_reps_10b_10r.RData")
out4 <- readRDS("trained_25_26_sets_3_reps_10b_10r.RData")

out1$best_models_valid
out2$best_models_valid
out3$best_models_valid
out4$best_models_valid

```

```{r}
runtimes1 <- -round(out1$time_df$timestamp[1:9] - out1$time_df$timestamp[2:10])
h2o_exp1 <- h2o_experiment(out_object = out1, runtimes = runtimes1, dataset = binary_CC18[1:3], reps = 3, limit = 99999)
saveRDS(h2o_exp1, file = "h2o_1_3_sets.RData")
h2o_exp1
```

```{r}
runtimes2 <- -round(out2$time_df$timestamp[1:6] - out2$time_df$timestamp[2:7])
h2o_exp2 <- h2o_experiment(out_object = out2, runtimes = runtimes2, dataset = binary_CC18[4:5], reps = 3, limit = 99999)
saveRDS(h2o_exp2, file = "h2o_4_5_sets.RData")
h2o_exp2
```

```{r}
runtimes3 <- -round(out3$time_df$timestamp[1:3] - out3$time_df$timestamp[2:4])
h2o_exp3 <- h2o_experiment(out_object = out3, runtimes = runtimes3, dataset = binary_CC18[19], reps = 3, limit = 99999)
saveRDS(h2o_exp3, file = "h2o_19_sets.RData")
h2o_exp3
```

```{r}
runtimes4 <- -round(out4$time_df$timestamp[1:6] - out4$time_df$timestamp[2:7])
h2o_exp4 <- h2o_experiment(out_object = out4, runtimes = runtimes4, dataset = binary_CC18[25:26], reps = 3, limit = 99999)
saveRDS(h2o_exp4, file = "h2o_25_26_sets.RData")
h2o_exp4
```


# Visualization 

This code section is responsible for creating the visualizations presented in the paper.

```{r}
library(ggplot2)
library(dplyr)
library(ggbeeswarm)
library(stringr)
```

```{r}
forester_1_3   <- readRDS("trained_1_3_sets_3_reps_10b_10r.RData")
forester_4_5   <- readRDS("trained_4_5_sets_3_reps_10b_10r.RData")
forester_19    <- readRDS("trained_19_sets_3_reps_10b_10r.RData")
forester_25_26 <- readRDS("trained_25_26_sets_3_reps_10b_10r.RData")

h2o_1_3   <- readRDS("h2o_1_3_sets.RData")
h2o_4_5   <- readRDS("h2o_4_5_sets.RData")
h2o_19    <- readRDS("h2o_19_sets.RData")
h2o_25_26 <- readRDS("h2o_25_26_sets.RData")

names(h2o_4_5$best_models_valid) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
names(h2o_19$best_models_valid) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
names(h2o_25_26$best_models_valid) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')

names(h2o_4_5$best_models_train) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
names(h2o_19$best_models_train) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
names(h2o_25_26$best_models_train) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')

names(h2o_4_5$best_models_test) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
names(h2o_19$best_models_test) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
names(h2o_25_26$best_models_test) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
```

```{r}
train_forester <- rbind(forester_1_3$best_models_train, forester_4_5$best_models_train, forester_19$best_models_train, forester_25_26$best_models_train)
train_forester <- train_forester[, c(8, 9, 5, 6, 7)]
train_forester$framework <- rep('forester', 24)

train_h2o <- rbind(h2o_1_3$best_models_train, h2o_4_5$best_models_train, h2o_19$best_models_train, h2o_25_26$best_models_train)
train_h2o$framework <- rep('h2o', 24)
train_performance <- rbind(train_forester, train_h2o)
train_performance

test_forester <- rbind(forester_1_3$best_models_test, forester_4_5$best_models_test, forester_19$best_models_test, forester_25_26$best_models_test)
test_forester <- test_forester[, c(8, 9, 5, 6, 7)]
test_forester$framework <- rep('forester', 24)

test_h2o <- rbind(h2o_1_3$best_models_test, h2o_4_5$best_models_test, h2o_19$best_models_test, h2o_25_26$best_models_test)
test_h2o$framework <- rep('h2o', 24)
test_performance <- rbind(test_forester, test_h2o)
test_performance

valid_forester <- rbind(forester_1_3$best_models_valid, forester_4_5$best_models_valid, forester_19$best_models_valid, forester_25_26$best_models_valid)
valid_forester <- valid_forester[, c(8, 9, 5, 6, 7)]
valid_forester$framework <- rep('forester', 24)

valid_h2o <- rbind(h2o_1_3$best_models_valid, h2o_4_5$best_models_valid, h2o_19$best_models_valid, h2o_25_26$best_models_valid)
valid_h2o$framework <- rep('h2o', 24)
valid_performance <- rbind(valid_forester, valid_h2o)
valid_performance
```

```{r}
train_performance$set <- "train"
test_performance$set <- "test"
valid_performance$set <- "valid"

df <- rbind(test_performance, train_performance, valid_performance)
df$set <- factor(df$set, levels = c("test", "valid", "train"))



df <- mutate(df, task_name_short = str_wrap(task_name, width = 8))

ggplot(data = df, aes(x = set, y = accuracy, color = framework)) + 
  geom_beeswarm(alpha = 0.9, size = 2) + 
  theme_minimal() + 
  labs(title = 'Performance comparison of forester and h2o',
       x = 'Dataset',
       y = 'Accuracy',
       color = "Framework",
       shape = "Set") +
  scale_color_manual(values = c("#afc968", "#74533d", "#2aa8b2")) +
  theme(plot.title = element_text(colour = 'black', size = 15),
        axis.title.x = element_text(colour = 'black', size = 12),
        axis.title.y = element_text(colour = 'black', size = 12),
        axis.text.y = element_text(colour = "black", size = 6),
        axis.text.x = element_text(colour = "black", size = 9)) + 
  ylim(c(0.5, 1)) + 
  coord_flip() + 
  facet_grid(task_name_short~.,) +
  theme(strip.background = element_rect(fill = "white", color = "white"),
        strip.text = element_text(size = 6 ), 
        legend.position = "bottom",
        strip.text.y.right = element_text(angle = 0)) 

```

# Raw training results

This code section is responsible for generating the data sets present in the raw_training_results and the tables present in the paper's Appendix.

```{r}
library(xlsx)
library(dplyr)
```

```{r}
forester_1_3   <- readRDS("C:/Users/Hubert/Documents/forester/trained_1_3_sets_3_reps_10b_10r.RData")
forester_4_5   <- readRDS("C:/Users/Hubert/Documents/forester/trained_4_5_sets_3_reps_10b_10r.RData")
forester_19    <- readRDS("C:/Users/Hubert/Documents/forester/trained_19_sets_3_reps_10b_10r.RData")
forester_25_26 <- readRDS("C:/Users/Hubert/Documents/forester/trained_25_26_sets_3_reps_10b_10r.RData")

h2o_1_3        <- readRDS("C:/Users/Hubert/Documents/forester/h2o_1_3_sets.RData")
h2o_4_5        <- readRDS("C:/Users/Hubert/Documents/forester/h2o_4_5_sets.RData")
h2o_19         <- readRDS("C:/Users/Hubert/Documents/forester/h2o_19_sets.RData")
h2o_25_26      <- readRDS("C:/Users/Hubert/Documents/forester/h2o_25_26_sets.RData")
```

```{r}
rename_cols <- function(df) {
  names(df$best_models_valid) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
  names(df$best_models_train) <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
  names(df$best_models_test)  <- c('task_name', 'rep', 'accuracy', 'auc', 'f1')
  return(df)
}

unify_and_merge <- function(a, b, c, d, type, col) {
  a  <- a[[col]]
  b  <- b[[col]]
  c  <- c[[col]]
  d  <- d[[col]]
  df <- rbind(a, b, c, d)
  
  if (type == 'forester') {
    df <- df[, c(8, 9, 5, 6, 7)]
  }
  df$framework <- rep(type, 24)
  
  return(df)
}

aggregate_df <- function(df) {
  df_mean <- df %>%
    group_by(task_name, framework) %>%
    summarise(mean_accuracy = mean(accuracy), mean_auc = mean(auc), mean_f1 = mean(f1))
  
  df_mean               <- data.frame(df_mean)
  df_mean$mean_accuracy <- round(df_mean$mean_accuracy, 3)
  df_mean$mean_auc      <- round(df_mean$mean_auc , 3)
  df_mean$mean_f1       <- round(df_mean$mean_f1, 3)
  df <- df_mean
  
  return(df)
}
```

```{r}
h2o_1_3        <- rename_cols(h2o_1_3)
h2o_4_5        <- rename_cols(h2o_4_5)
h2o_19         <- rename_cols(h2o_19)
h2o_25_26      <- rename_cols(h2o_25_26)

forester_train <- unify_and_merge(forester_1_3, forester_4_5, forester_19, forester_25_26, 'forester', 1)
forester_test  <- unify_and_merge(forester_1_3, forester_4_5, forester_19, forester_25_26, 'forester', 2)
forester_valid <- unify_and_merge(forester_1_3, forester_4_5, forester_19, forester_25_26, 'forester', 3)

h2o_train      <- unify_and_merge(h2o_1_3, h2o_4_5, h2o_19, h2o_25_26, 'h2o', 1)
h2o_test       <- unify_and_merge(h2o_1_3, h2o_4_5, h2o_19, h2o_25_26, 'h2o', 2)
h2o_valid      <- unify_and_merge(h2o_1_3, h2o_4_5, h2o_19, h2o_25_26, 'h2o', 3)

train          <- rbind(forester_train, h2o_train)
test           <- rbind(forester_test, h2o_test)
valid          <- rbind(forester_valid, h2o_valid)

train_mean     <- aggregate_df(train)
test_mean      <- aggregate_df(test)
valid_mean     <- aggregate_df(valid)
```

```{r}
write.xlsx(train_mean, "C:/Users/Hubert/Documents/forester/train_mean.xlsx", row.names = FALSE)
write.xlsx(test_mean, "C:/Users/Hubert/Documents/forester/test_mean.xlsx", row.names = FALSE)
write.xlsx(valid_mean, "C:/Users/Hubert/Documents/forester/valid_mean.xlsx", row.names = FALSE)

write.csv(train_mean, "C:/Users/Hubert/Documents/forester/train_mean.csv", row.names = FALSE)
write.csv(test_mean, "C:/Users/Hubert/Documents/forester/test_mean.csv", row.names = FALSE)
write.csv(valid_mean, "C:/Users/Hubert/Documents/forester/valid_mean.csv", row.names = FALSE)

write.xlsx(train, "C:/Users/Hubert/Documents/forester/train.xlsx", row.names = FALSE)
write.xlsx(test, "C:/Users/Hubert/Documents/forester/test.xlsx", row.names = FALSE)
write.xlsx(valid, "C:/Users/Hubert/Documents/forester/valid.xlsx", row.names = FALSE)

write.csv(train, "C:/Users/Hubert/Documents/forester/train.csv", row.names = FALSE)
write.csv(test, "C:/Users/Hubert/Documents/forester/test.csv", row.names = FALSE)
write.csv(valid, "C:/Users/Hubert/Documents/forester/valid.csv", row.names = FALSE)
```


