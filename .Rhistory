y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification",
verbose = FALSE)
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed,
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification",
verbose = FALSE)
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed,
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification",
verbose = FALSE)
View(catboost_explain)
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed,
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
plot(model_parts(catboost_explain))
cat_data <- catboost.load_pool(titanic_imputed[, -which(names(titanic_imputed) == "survived")], titanic_imputed$survived)
cat_model <- catboost.train(data,  NULL )
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed[, -which(names(titanic_imputed) == "survived")],
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed[, -which(names(titanic_imputed) == "survived")],
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed,
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
plot(model_parts(catboost_explain))
titanic_imputed[,-"survived"]
cat_data <- catboost.load_pool(titanic_imputed[,-c("survived")], titanic_imputed$survived)
titanic_imputed[, -which(names(titanic_imputed) == "survived")]
cat_data <- catboost.load_pool(titanic_imputed[, -which(names(titanic_imputed) == "survived")], titanic_imputed$survived)
cat_model <- catboost.train(cat_data,  NULL )
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed[, -which(names(titanic_imputed) == "survived")],
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
plot(model_parts(catboost_explain))
cat_data2 <- catboost.load_pool(dragons[, -which(names(titanic_imputed) == "scars")],
dragons$scars)
cat_data2 <- catboost.load_pool(dragons[, -which(names(dragons) == "scars")],
dragons$scars)
cat_data2 <- catboost.load_pool(dragons[, -which(names(dragons) == "scars")],
dragons$scars)
cat_model <- catboost.train(cat_data2,  NULL )
catboost_explain <- DALEX::explain(cat_model,
data = dragons[, -which(names(dragons) == "scars")],
y = dragons$scars,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
plot(model_parts(catboost_explain))
# catboost wrapper
catboost_predict <- function(object, newdata) {
newdata_pool <- catboost.load_pool(newdata)
return( catboost.predict(object, newdata_pool, prediction_type = "Class"))
}
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed[, -which(names(titanic_imputed) == "survived")],
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
Probability
# catboost wrapper
catboost_predict <- function(object, newdata) {
newdata_pool <- catboost.load_pool(newdata)
return( catboost.predict(object, newdata_pool, prediction_type = "Probability"))
}
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed[, -which(names(titanic_imputed) == "survived")],
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
# catboost wrapper
catboost_predict <- function(object, newdata) {
newdata_pool <- catboost.load_pool(newdata)
return( catboost.predict(object, newdata_pool, prediction_type = "Probability"))
}
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed[, -which(names(titanic_imputed) == "survived")],
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
cat_data <- catboost.load_pool(titanic_imputed[, -which(names(titanic_imputed) == "survived")], titanic_imputed$survived)
cat_model <- catboost.train(cat_data,  NULL )
catboost_explain <- DALEX::explain(cat_model,
data = titanic_imputed[, -which(names(titanic_imputed) == "survived")],
y = titanic_imputed$survived,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "classification")
cat_data2 <- catboost.load_pool(dragons[, -which(names(dragons) == "scars")],
dragons$scars)
cat_model <- catboost.train(cat_data2,  NULL )
catboost_explain <- DALEX::explain(cat_model,
data = dragons[, -which(names(dragons) == "scars")],
y = dragons$scars,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "regression")
plot(model_parts(catboost_explain))
# catboost wrapper
catboost_predict <- function(object, newdata) {
newdata_pool <- catboost.load_pool(newdata)
return( catboost.predict(object, newdata_pool, prediction_type = "Probability"))
}
catboost_explain <- DALEX::explain(cat_model,
data = dragons[, -which(names(dragons) == "scars")],
y = dragons$scars,
label = "CatBoost (R)",
predict_function = catboost_predict,
type = "regression")
plot(model_parts(catboost_explain))
modelStudio::modelStudio(catboost_explain)
modelStudio::modelStudio(catboost_explain)
make_xgboost <- function(data,target,type="regression")
{
### Conditions:
# Check data class
if  (! any(class(data) %in% c("data.frame","dgCMatrix","matrix","data.table")))
stop("Object is not one of the types: 'data.frame','dgCMatrix','matrix','data.table")
# Unify data class to data frame
if (any(class(data)=="matrix"))
{data <- as.data.frame(data)}
if (any(class(data)=="data.table"))
{data <- as.data.frame(data)}
if (any(class(data)=="dgCMatrix"))
{data <- as.data.frame(as.matrix(data))}
# Checking if data frame is empty
if (nrow(data) == 0 | ncol(data) < 2) {
stop("The data frame is empty or has too little columns.")}
# Check if target is one of colnames of data frame
if ( !is.character(target) | !(target %in% colnames(data)))
stop("Either 'target' input is not a character or data does not have column named similar to 'target'")
### Data processing level 1/2 (remove NAs, split label and training frame,...)
# Remove rows with NA values (I will write in the documentation of function):
data <- na.omit(data)
# Change character columns to factors
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)],as.factor)
# Extract the target vector from our dataframe:
label_column <- data[[target]]
# Check condition for type between "classification" and "regression":
if ((type != "classification")&(type!="regression"))
stop("Type of problem is invalid.")
### Conditions and processing on target column:
# Binary Classification
if (type=="classification")
{
# Checking number of classes in target column:
if (length(unique(label_column))< 2)
{stop("Too few classes for binary classification")}
if (length(unique(label_column))>2)
{stop("Too many classes for binary classification")}
# Coercing the target from factor to numeric values:
if (class(label_column)=="factor")
{label_column <- as.numeric(data[[target]]==data[1,target])
message("Program coerces factors into numeric values:")
var_true <- as.character(data[1,target])
var_all  <- as.character(unique(data[,target]))
message(paste("MECHANISM: ",var_true," -> 1 and ",var_all[! var_all  %in% var_true]," -> 0."))
}
# Rescaling target columns to be in [0,1]
uniq_vals <- unique(label_column)
if ((min(uniq_vals)!=0)|(max(uniq_vals)!=1))
{
# Change max -> 1; min -> 0:
max <- max(uniq_vals)
min <- min(uniq_vals)
message("Two values in target column are not in [0,1]")
message(paste("MECHANISM: ",min ," -> 0 and", max,"-> 1."))
label_column[label_column== max] <- 1
label_column[label_column==min]  <- 0
}
}
# Regression:
if (type == "regression")
{
if (class(label_column)=="factor")
{stop("Program is stopped. The class of target column is factor, not appropriate for regression problem")}
# Double-check for numeric type of label column:
label_column <- as.numeric(label_column)
}
### Data processing level 2/2
# Data frame without target
data_info <- data[,!(colnames(data) %in% target),drop=FALSE]
# One-hot encoding for categorical features in Data frame without target
names_factor <- colnames(data_info)[sapply(data_info,is.factor)] # finding names of categorical attributes
vector_index <- which(colnames(data_info) %in% names_factor)     # positions of those categorical attributes
# in colnames of Data frame without target
data_encoded <- model.matrix(~.-1,data=data_info,contrasts.arg = lapply(data_info[,vector_index,drop=FALSE],
contrasts,contrasts=FALSE))
# using model.matrix to create one-hot encoded matrix
# Transform from data frame type to matrix to prepare for XGBboost model:
data_encoded_matrix <- data.matrix(data_encoded)
# Convert to object used in XGBoost model:
dtrain <- xgboost::xgb.DMatrix(data=data_encoded_matrix,label=label_column)
### XGBoost Model
# Regression
if (type=="regression"){
model <- xgboost::xgb.train(data=dtrain, verbose = 0,
max.depth=8, eta=0.3,
nrounds=4, nthread=2,
objective= "reg:linear")
}
# Binary classification
if (type=="classification"){
# In this step: thinking of offering user to choose return classes (0 or 1/ value of numeric target) or probability.
model <- xgboost::xgb.train(data=dtrain, verbose = 0,
max.depth=8, eta=0.3,
nrounds=4, nthread=2,
eval_metric= "logloss",
objective= "binary:logistic")
}
### Explainer from DALEX
# For simplicity, take processed matrix from original data frame for explanation purpose:
explainer_automate_xgb <- DALEX::explain(model,data=data_encoded_matrix,y=label_column,label="XGBoost")
return(explainer_automate_xgb)
}
library(xgboost)
library(DALEX)
exp <- make_xgboost(iris, "pental_lenght", "regression")
head(iris)
exp <- make_xgboost(iris, "Species", "classification")
?mtcars
exp <- make_xgboost(mtcars, "mpg", "regression")
# Making explanations
plot(model_parts(exp))
make_ranger <- function(data, target, type) {
### Conditions
# Checking data class
if (!(any(class(data) != 'data.frame') | any(class(data) != 'matrix')
| any(class(data) != 'dgCMatrix') | any(class(data) != 'data.table'))){
stop("Wrong data type")
}
# Changing data to normal data frame
if (any(class(data) != "data.frame")){
if (any(class(data) == 'dgCMatrix')){data <- as.data.frame(as.matrix(data))}
else if (any(class(data) == 'matrix')){data <- as.data.frame(data)}
else if (any(class(data) == 'data.table')){data <- as.data.frame(data)}
}
# Checking if data frame is empty
if (nrow(data) == 0 | ncol(data) < 2) {
stop("The data frame is empty or has too little columns.")
}
# Checking if target column is in data frame
if (typeof(target) != 'character' | !(target %in% colnames(data))) {
stop("Incorrect target value")
}
# Checking if type is correct
if (typeof(type) != 'character' | (type != "classif" & type != "regr")){
stop("Incorrect type name. Choose beetwen 'classif' and 'regr'. ")
}
# Creating formula
# Checking if target value has nans
if (any(is.na(data[, target]))){
stop("There can't be NaN values in target column!")
}
# try to convert column names to names without znaki
test_colnames<- lapply(colnames(data),function(x) gsub("_", "", x))
test_colnames<- lapply(colnames(data),function(x) gsub("[.]", "", x))
if(any(grepl('[[:punct:]]', test_colnames))) {
colnames(data) <- lapply(colnames(data),function(x) gsub("[[:punct:]]", "_", x))
target <- gsub("[[:punct:]]", "_", target)
message("Column names are wrong for creating a formula. Replacing special signs with '_'")
}
form <- stats::as.formula(paste(target , "~."))
### Model
# First binary classification
if (type == "classif") {
# Checking if theres right number of classes in target column
if (length(unique(data[, target])) < 2) {
stop("To little classes for binary classification")
} else if (length(unique(data[, target])) > 2){
stop("To many classes for binary classification")
}
if (class(data[[target]]) != "numeric") {
# Converting target column to numeric for Dalex
uniq <- unique(data[, target])
data[, target] <- ifelse(data[, target] == uniq[1], 0, 1)
message(paste("Wrong type of target column. Changed to numeric: ",
uniq[1], " -> 1 and ", uniq[2], " -> 0 ", sep=""))
}
# Creating model
rg <- ranger::ranger(form, data = data, classification = TRUE)
} else {
# Checking if target column is numeric
if (class(data[[target]]) != 'numeric' & class(data[[target]]) != 'integer') {
stop("Target value should be numeric for regression task")
}
#Creating a model
rg <- ranger::ranger(form, data = data)
}
### Explainer
# Deleting target column from data frame
df_exp <- data[, -which(names(data) == target)]
# Creating an explainer
ranger_explained <- DALEX::explain(rg,
data = df_exp,
y = data[, target],
label = "Ranger")
return(ranger_explained)
}
make_light_gbm <- function(data,target,type="regression")
{
### Conditions:
# Check data class
if  (! any(class(data) %in% c("data.frame","dgCMatrix","matrix","data.table")))
stop("Object is not one of the types: 'data.frame','dgCMatrix','matrix','data.table")
# Unify data class to data frame
if (any(class(data)=="matrix"))
{data <- as.data.frame(data)}
if (any(class(data)=="data.table"))
{data <- as.data.frame(data)}
if (any(class(data)=="dgCMatrix"))
{data <- as.data.frame(as.matrix(data))}
# Checking if data frame is empty
if (nrow(data) == 0 | ncol(data) < 2) {
stop("The data frame is empty or has too little columns.")}
# Check if target is one of colnames of data frame
if ( !is.character(target) | !(target %in% colnames(data)))
stop("Either 'target' input is not a character or data does not have column named similar to 'target'")
### Data processing level 1/2 (remove NAs, split label and training frame,...)
# Remove rows with NA values (I will write in the documentation of function):
data <- na.omit(data)
# Change character columns to factors
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)],as.factor)
# Extract the target vector from our dataframe:
label_column <- data[[target]]
# Check condition for type between "classification" and "regression":
## Classification type
if ((type != "classification")&(type!="regression"))
stop("Type of problem is invalid.")
### Conditions and processing on target column:
# Binary Classification
if (type=="classification")
{
if (length(unique(label_column))< 2)
{stop("Too few classes for binary classification")}
if (length(unique(label_column))>2)
{stop("Too many classes for binary classification")}
# Coercing the target from factor to numeric values:
if (class(label_column)=="factor")
{label_column <- as.numeric(data[[target]]==data[1,target])
message("Program coerces factors into numeric values:")
var_true <- as.character(data[1,target])
var_all  <- as.character(unique(data[,target]))
message(paste("MECHANISM: ",var_true," -> 1 and ",var_all[! var_all  %in% var_true]," -> 0."))
}
# Rescaling target columns to be in [0,1]
uniq_vals <- unique(label_column)
if ((min(uniq_vals)!=0)|(max(uniq_vals)!=1))
{
# Change max -> 1; min -> 0:
max <- max(uniq_vals)
min <- min(uniq_vals)
message("Two values in target column are not in [0,1]")
message(paste("MECHANISM: ",min ," -> 0 and", max,"-> 1."))
label_column[label_column== max] <- 1
label_column[label_column==min]  <- 0
}
}
# Regression:
if (type == "regression")
{
if (class(label_column)=="factor")
{stop("Program is stopped. The class of target column is factor, not appropriate for regression problem")}
# Double-check for numeric type of label column:
label_column <- as.numeric(label_column)
}
### Data processing level 2/2
# Data frame without target
data_info <- data[,!(colnames(data) %in% target),drop=FALSE]
# Using lgb.convert_with_rules. The function transforms the data into a fittable data
data_info_rules <-lightgbm::lgb.convert_with_rules(data=data_info)
data_encoded <- data_info_rules$data
# Transform from dataframe to matrix:
data_encoded_matrix <- as.matrix(data_encoded)
# Convert to object used in LightGBM model:
names_cat_vars <- names(data_info_rules$rules)
dtrain <- lightgbm::lgb.Dataset(data=data_encoded_matrix,label=label_column,
categorical_feature = as.vector(which(colnames(data_encoded) %in% names_cat_vars)))
### XGBoost Model
# Regression
if (type=="regression"){
model <- lightgbm::lightgbm(data=dtrain, verbose = -1,
learning_rate=0.7,
nrounds = 10,objective="regression")
}
# Binary classification
if (type=="classification"){
# In this step: thinking of offering user to choose return classes (0 or 1/ value of numeric target) or probability.
model <- lightgbm::lightgbm(data=dtrain, verbose = -1,
learning_rate = 0.4,
nrounds = 10,
objective = "binary")
}
### Explainer from DALEX
# For simplicity, take processed matrix from original data frame for explanation purpose:
explainer_automate_lightgbm <- DALEX::explain(model,data=data_encoded_matrix,y=label_column,label="LightGBM")
return(explainer_automate_lightgbm)
}
library(lightgbm)
library(ranger)
exp <- make_light_gbm(mtcars, "mpg", "regression")
# Making explanations
plot(model_parts(exp))
exp <- make_ranger(mtcars, "mpg", "regr")
# Making explanations
plot(model_parts(exp))
model_profile(exp, variable =  "disp", type = "partial")
plot(model_profile(exp, variable =  "disp", type = "partial"))
head(mtcars)
plot(model_profile(exp, variable =  "hp", type = "partial"))
plot(model_profile(exp, variable =  "drat", type = "partial"))
nobs <- mtcars[1, , drop = FALSE]
sp_xgb  <- predict_parts(exp,
new_observation = nobs,
type = "break_down")
plot(sp_xgb)
exp <- make_xgboost(mtcars, "mpg", "regression")
# Making explanations
plot(model_parts(exp))
plot(model_profile(exp, variable =  "drat", type = "partial"))
nobs <- mtcars[1, , drop = FALSE]
sp_xgb  <- predict_parts(exp,
new_observation = nobs,
type = "break_down")
plot(sp_xgb)
plot(model_profile(exp, variable =  "disp", type = "partial"))
# Making explanations
plot(model_parts(exp))
# Making explanations
plot(model_parts(exp), max_vars = 6)
# Making explanations
plot(model_parts(exp), max_vars = 4)
# Making explanations
plot(model_parts(exp), max_vars = 5)
exp <- make_ranger(mtcars, "mpg", "regr")
# Making explanations
plot(model_parts(exp), max_vars = 5)
exp <- make_ranger(mtcars, "mpg", "regr")
# Making explanations
plot(model_parts(exp), max_vars = 5)
nobs <- mtcars[1, , drop = FALSE]
sp_xgb  <- predict_parts(exp,
new_observation = nobs,
type = "break_down")
plot(sp_xgb)
# Making explanations
plot(model_parts(exp), max_vars = 5)
plot(sp_xgb)
# Making explanations
plot(model_parts(exp), max_vars = 5)
plot(sp_xgb)
nobs <- mtcars[1, , drop = FALSE]
View(nobs)
nobs <- mtcars[1, ]
nobs <- mtcars[1, ]
available::available("forester")
available::available("forester")
install.packages(available)
install.packages('available')
available::available("forester")
getwd()
setwd("/Users/szymonszmajdzinski/R-Projects/forester/")
devtools::check()
devtools::uses_testthat()
devtools::use_testthat()
use_testthat
use_testthat()
usethis::use_testthat()
use_test()
usethis::use_test()
setwd("/Users/szymonszmajdzinski/Documents/GitHub/forester/")
devtools::check()
devtools::check()
usethis::use_testthat()
usethis::uses_testthat()
uses_testthat()
usethis::use_testthat()
usethis::use_test()
devtools::check()
library(testthat)
detach("package:testthat", unload = TRUE)
library(usethis)
detach("package:usethis", unload = TRUE)
usethis::use_testthat()
