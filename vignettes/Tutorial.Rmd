---
title: "forester tutorial"
author: "Hubert RuczyÅ„ski"
output:
  pdf_document: default
  df_print: paged
  word_document: default
  html_document: null
toc: yes
toc_float: yes
number_sections: yes
vignette: >
  %\VignetteIndexEntry{tutorial} 
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown} 
---
# forester

In this short tutorial you will get to know how to use the forester `package`.
First of all, you don't need any initial knowledge to benefit from computational
power of the tree-based models, because forester can do every crucial step for
you. 

## Data

The data set used in our tutorial is called `lisbon` It contains 17 columns with
both numerical and categorical data about real estates in Lisbon. Our target 
here is a `Price` column and the task is the regression. The only thing we need
to start working with this task is the `forester` package, however to explain the
model we will also use `DALEX` package.

```{r eval = TRUE,warning = FALSE, message = FALSE, results = 'hide', imports}
library(DALEX)
library(forester)
```

```{r eval = TRUE, head-lisbon}
data('lisbon')
knitr::kable(head(lisbon, 10), format = "markdown", digits = 3, align = 'c')
```

## Train method

The main function inside the package is `train()` function. It needs only two
parameters to work: `data` and `y`, which are the data set and target column name.
This method automatically checks the data set for possible issues, fixes the ones
that are needed for models to be built in preprocessing process and trains the models
with basic parameters, randomly searched parameters, and the ones chosen by Bayesian
optimization. It returns an advanced object, but the most important for casual
users is the ranked list of all trained models sorted from the best one to the
worst. Moreover, it prints out what is currently happening during the whole process.

```{r eval = TRUE, warning = FALSE, train}
train_output <- train(lisbon, 
                      'Price', 
                      engine = c('ranger', 'xgboost', 'decision_tree', 'lightgbm'),
                      bayes_iter = 1, 
                      random_evals = 2, 
                      advanced_preprocessing = TRUE)
```

```{r eval = TRUE, ranked_list}
knitr::kable(train_output$ranked_list, format = 'markdown', digits = 3, format.args = list(big.mark = ',', scientific = FALSE), align = 'c')
```
## Explanations

To access a single model the user has to get to the models_list and then choose the
exact models that interest him. As we want to create a `DALEX explainer` and 
Feature Importance plot, as an example we will use the first model `xgboost`,
because unfortunately, we are unable to create this visualization for catboost models
yet.

```{r eval = TRUE, explainer}
explainer <- forester::explain(models = train_output$best_models$xgboost_model, 
                               test_data = train_output$test_data, 
                               y = train_output$y, 
                               verbose = TRUE)
```

```{r eval = TRUE, FI}
FI <- DALEX::model_parts(explainer)
FI
```
```{r eval = TRUE, FI_plot_basic}
plot(FI)
```

However, this visualization is barely a basic one. To get the FI plot for the best 
model trained, one can use the `draw_feature_importance` function.

```{r eval = TRUE, FI_plot}
draw_feature_importance(train_output$best_models, train_output$train_data, train_output$y)
```

From the above visualization, we can see that the most important variables for the
`xgboost` model were `Bathrooms`, `Latitude`, and `Longitude`.

## Report

The aforementioned visualizations, like many others, are also available in ready-to-go
reports implemented in the `report` function.

```{r eval = TRUE, report}
report(train_output, 'regression.pdf')
```

