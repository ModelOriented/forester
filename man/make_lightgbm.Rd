% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/make_lightgbm.R
\name{make_lightgbm}
\alias{make_lightgbm}
\title{Automated Function for Explaining LightGBM model}
\usage{
make_lightgbm(
  data,
  target,
  type,
  num_features = NULL,
  fill_na = TRUE,
  tune = FALSE,
  tune_metric = NULL,
  tune_iter = 20,
  label = "LightGBM"
)
}
\arguments{
\item{data}{data.frame, matrix, data.table or dgCMatrix - data will be used to run the XGBoost model. NOTE: data has to contain the target column.}

\item{target}{character: name of the target column, should be character and has to be a column name in data.}

\item{type}{character: defining the task. Two options are: "regression" and "classification", particularly, binary classification.}

\item{num_features}{numeric, default is NULL. Parameter indicates number of most important features, which are chosen from the train dataset. Automatically, those important
features will be kept in the train and test datasets.}

\item{fill_na}{logical, default is FALSE. If TRUE, missing values in target column are removed, missing values in categorical columns are replaced by mode and
missing values in numeric columns are substituted by median of corresponding columns.}

\item{tune}{logical. If TRUE, function will perform the hyperparameter tuning steps for each model inside.}

\item{tune_metric}{character, name of metric used for evaluating best model. For regression, options are: "mse", "rmse", "mad" and "r2".
For classification, options are: "auc", "recall", "precision", "f1" and "accuracy".}

\item{tune_iter}{number (default: 20) - total number of times the optimization step is to repeated. This argument is used when tune = TRUE.}

\item{label}{string indicating the name of the model. Might be usefull while comparing different models of the same type.}
}
\value{
An object of the class \code{explainer} for LightGBM model with given data, target and defined type of problem.
}
\description{
Function \code{make_lightgbm} automates the process of applying LightGBM model
for a dataset and simutaneously creates explainer from the use of DALEX package.
The created explainer can be further processed by functions for explanations.
}
\examples{
# simple explainer for regression problem

library(datasets)
library(DALEX)
data("iris")
EXPLAINER <- make_lightgbm(iris,"Petal.Length",type="regression")

## Variable importance
variable_importance_gbm <- model_parts(EXPLAINER,type="raw")
plot(variable_importance_gbm)

## Single variable:
sv_gbm_petal_width <- model_profile(EXPLAINER, variable="Petal.Width",type="partial")
plot(sv_gbm_petal_width)


# simple explainer for classification problem

library(DALEX)
## We will predict the survived state: 0 and 1
titanic_explainer <- make_lightgbm(titanic_imputed, "survived", "classification")

## Assessment for model performance:
model_performance(titanic_explainer)

## Finding important features:
plot(model_parts(titanic_explainer))

## Plot reverse cumulative distribution of module of residual
plot(model_performance(titanic_explainer))

}
\references{
Explanatory Model Analysis. Explore, Explain and Examine Predictive Models. \url{https://ema.drwhy.ai/}
}
