% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train_models_bayesopt.R
\name{train_models_bayesopt}
\alias{train_models_bayesopt}
\title{Train models with Bayesian Optimization algorithm}
\usage{
train_models_bayesopt(
  train_data,
  y,
  time,
  status,
  test_data,
  engine,
  type,
  parallel = FALSE,
  iters.n = 7,
  bayes_info = list(verbose = 0, plotProgress = FALSE),
  return_params = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{train_data}{A training data for models created by `prepare_data()` function.}

\item{y}{A string that indicates a target column name for regression or classification.
Either y, or pair: time, status can be used.}

\item{time}{A string that indicates a time column name for survival analysis task.
Either y, or pair: time, status can be used.}

\item{status}{A string that indicates a status column name for survival analysis task.
Either y, or pair: time, status can be used.}

\item{test_data}{A test data for models created by `prepare_data()` function.}

\item{engine}{A vector of tree-based models that shall be created. Possible
values are: `ranger`, `xgboost`,`decision_tree`, `lightgbm`, `catboost`. Doesn't
matter for survival analysis.}

\item{type}{A string that determines if Machine Learning task is the
`binary_clf`, `regression`, or `survival`.}

\item{parallel}{A logical value, if set to TRUE, the function will use parallel computing.
By default set to FALSE.}

\item{iters.n}{The number of iterations of BayesOpt function.}

\item{bayes_info}{A list with two values, determining the verbosity of the Bayesian
Optmization process. The first value is `verbose` with 3 levels: 0 - no output;
1 - describes what is hapenning, and if we can reach local optimum; 2 - addtionally
provides infromation about recent, and the best scores. The second value is
`plotProgress`, which is a logical value indicating if the progress of the Bayesian
Optimization should be plotted. WARNING it will create plot after each step, thus
it might be computationally expensive. Both arguments come from the
`ParBayesianOptimization` package. It only matters if you set global verbose to TRUE.
Default values are: list(verbose = 0, plotProgress = FALSE).}

\item{return_params}{A logical value, if set to TRUE, returns optimized model parameters.}

\item{verbose}{A logical value, if set to TRUE, provides all information about
the process, if FALSE gives none.}
}
\value{
Trained models with optimized parameters. If `return_params` is `TRUE`, then
returns also training parameters in the one list with models.
}
\description{
Bayesian Optimization takes relatively a long time - the bigger `iters.n` param,
the more time (but if you want to get model parameters better than default params,
it is suggested to set `iters.n` equals 20 at least.
Also the bigger dataset, the more time takes Bayesian Optimization.
}
